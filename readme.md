# ä¸€ä¸ªè‡ªå·±å†™çš„ PyTorch ç®€å•æ¡†æ¶èŒƒä¾‹

ä½¿ç”¨ Pytorch å®ç°ç¥ç»ç½‘ç»œæ¨¡å‹çš„ä¸€èˆ¬æµç¨‹åŒ…æ‹¬ï¼š

1. å‡†å¤‡æ•°æ®
2. å®šä¹‰æ¨¡å‹
3. è®­ç»ƒæ¨¡å‹
4. è¯„ä¼°æ¨¡å‹
5. ä½¿ç”¨æ¨¡å‹
6. ä¿å­˜æ¨¡å‹

**å¯¹æ–°æ‰‹æ¥è¯´ï¼Œå…¶ä¸­æœ€å›°éš¾çš„éƒ¨åˆ†å®é™…ä¸Šæ˜¯å‡†å¤‡æ•°æ®è¿‡ç¨‹ã€‚**

å¦å¤–ä¸€æ–¹é¢ï¼ŒPytorch é€šå¸¸éœ€è¦ç”¨æˆ·ç¼–å†™è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œè®­ç»ƒå¾ªç¯çš„ä»£ç é£æ ¼å› äººè€Œå¼‚ã€‚

ç°åœ¨å¸‚é¢ä¸Šä¸ä¹å¯¹ PyTorch è¿›è¡ŒäºŒæ¬¡åŒ…è£…çš„æ¡†æ¶ï¼Œä¾‹å¦‚ï¼š[Lightning](https://www.pytorchlightning.ai/)ï¼Œhuggingfaceã€‚è¯šç„¶è¿™äº› Trainer åŠŸèƒ½è¶³å¤Ÿå¼ºå¤§ï¼Œä¸ºäº†å¸å¼•æ›´å¤šäººä½¿ç”¨ï¼ŒåŠ å…¥å°½å¯èƒ½å¤šçš„åŠŸèƒ½ï¼Œæ¯”å¦‚åŸºæœ¬çš„æ—¥å¿—ã€tensorboardã€æ–­ç‚¹é‡è®­ã€è®­ç»ƒæ—¶éªŒè¯ç­‰ã€‚

åœ¨æœ€å¼€å§‹çš„æ—¶å€™æˆ‘ä¹Ÿå°è¯•è¿‡å»ä½¿ç”¨å®ƒæ¥å†™æ¨¡å‹ï¼Œç„¶è€Œå¤§å¤šæ•°æ—¶å€™æˆ‘ä»¬å¹¶ä¸éœ€è¦è¿™ä¹ˆå¤šçš„åŠŸèƒ½ï¼Œæˆ‘ä»¬ä¸éœ€è¦å„ç§å„æ ·çš„æ¥å£ï¼Œæœ‰æ—¶å€™ç”šè‡³ä¸å¦‚è‡ªå·±å†™ä¸€ä¸ª Loopï¼Œæ¯•ç«Ÿæˆ‘èƒ½ç¡®ä¿æˆ‘æ¸…æ¥šæ‰€æœ‰çš„æµç¨‹ï¼Œä¹Ÿèƒ½å¤Ÿåœ¨å¿…è¦çš„æ—¶å€™è¿›è¡Œé€‚å½“çš„â€œé­”æ”¹â€ã€‚

ä½†æ˜¯ä¸€ä¸ªç®€å•æ¸…æ™°çš„ Trainer ç¡®å®æœ‰ä¸å°‘å¥½å¤„ï¼Œèµ·ç ä»£ç çœ‹ç€ä¼˜ç¾å¤šäº†ï¼Œä¸‹é¢æ˜¯æˆ‘è‡ªå·±å†™æ¨¡å‹æ—¶å€™ä¸€ä¸ªç®€å•æ¡†æ¶èŒƒä¾‹ï¼Œå¹¶æ²¡æœ‰æ·»åŠ ä»€ä¹ˆä¹±ä¸ƒå…«ç³Ÿçš„å¤–éƒ¨åº“ï¼Œåªæ˜¯ç®€å•å®šä¹‰äº†ä¸€ä¸ª Trainer ç±»ï¼ŒåŒæ—¶å¯¹æ•´ä½“çš„è®­ç»ƒæµç¨‹è¿›è¡Œäº†æ€»ç»“ã€‚

å½“ç„¶ï¼Œè¿™åªæ˜¯æˆ‘è‡ªå·±ä¹ æƒ¯çš„ä¸€äº›å†™æ³•ï¼ˆå ç”²å ç”²ï¼‰ğŸ¤£

ä½ å¯ä»¥åœ¨ []() æ‰¾åˆ°å®Œæ•´çš„ä»£ç ã€‚

## Overview

```text
.
â”œâ”€â”€ Dataset/                    # æ•°æ®æ–‡ä»¶å¤¹
â”œâ”€â”€ Model/                      # æ¨¡å‹æ–‡ä»¶å¤¹
â”œâ”€â”€ main.py                     # main å‡½æ•°
â”œâ”€â”€ readme.md
â”œâ”€â”€ requirements.txt            # ç¯å¢ƒè¦æ±‚
â””â”€â”€ src
    â”œâ”€â”€ args.py                 # å‚æ•°é…ç½®
    â”œâ”€â”€ dataloader.py           # æ•°æ®åŠ è½½æ–‡ä»¶ï¼Œæ§åˆ¶æ‰€æœ‰æ•°æ®
    â”œâ”€â”€ dataset.py              # æ•°æ®ç»“æ„æ–‡ä»¶ï¼ŒåŒ…å«æ‰€æœ‰çš„è‡ªå®šä¹‰æ•°æ®é›†
    â”œâ”€â”€ model.py                # æ¨¡å‹æ–‡ä»¶ï¼ŒåŒ…å«æ‰€æœ‰çš„æ¨¡å‹
    â”œâ”€â”€ trainer.py              # trainer ç±»
    â””â”€â”€ utils.py                # å·¥å…·å‡½æ•°æ–‡ä»¶
```

ä¸‹é¢æˆ‘ç®€å•è¯´ä¸€ä¸‹æ¯ä¸ªæ–‡ä»¶éƒ½å¹²äº†ä»€ä¹ˆã€‚

## main å‡½æ•°

```py
import src.utils as utils
from src.args import get_args
from src.dataloader import Mydataloader
from src.model import MyModel
from src.trainer import Trainer

# load config
config = get_args()

# init logger
logger = utils.init_logger()

# init seed
utils.seed_everything(3407)


def main():
    logger.info(f'start loading data: {config.dataset} ...')
    my_loader = Mydataloader(config)
    my_loader.load()
    logger.info('load data done!')

    logger.info('start creating dataset ...')
    my_loader.create_dataset()
    logger.info('creating dataset done!')

    logger.info(f'start loading model ...')
    logger.info(f'mode: {config.mode}')
    model = MyModel(config)
    logger.info('load model done!')
    logger.info(f'epochs={config.epochs}, '
                f'batch_size={config.batch_size}, '
                f'hidden_size={config.hidden_size}')

    trainer = Trainer(config=config, logger=logger, gpu=config.gpu)
    if config.mode == 'train':
        trainer.train(model=model, dataloader=my_loader)
    else:
        trainer.test(model=model, dataloader=my_loader, model_path=config.model_path)


if __name__ == '__main__':
    main()
```

main å‡½æ•°å…¶å®éå¸¸ç®€å•ï¼Œå¤§è‡´ä¸Šä¹Ÿä¸éœ€è¦è¿›è¡Œä¿®æ”¹ï¼ŒåŸºæœ¬éƒ½æ˜¯é€šç”¨çš„ã€‚

1. é¦–å…ˆ`7-14`è¡Œï¼Œåˆ†åˆ«åˆ›å»ºäº† **å‚æ•°**ï¼ˆ`config`ï¼‰ï¼Œ**æ—¥å¿—** (`logger`) å¯¹è±¡ï¼Œå¹¶ä¸”æŒ‡å®šäº†éšæœºç§å­ã€‚
2. ç¤ºä¾‹åŒ–äº†`Mydataloader`ç±»ï¼ŒåŒæ—¶è°ƒç”¨äº†`load()`æ–¹æ³•å’Œ`create_dataset()`æ–¹æ³•ã€‚

> `Mydataloader` ç±»ä¸»è¦ç”¨äºæ§åˆ¶æ‰€æœ‰å’Œè¾“å…¥æ•°æ®ç›¸å…³çš„ä¸œè¥¿ï¼ŒåŒ…æ‹¬æ•°æ®çš„åŠ è½½ï¼Œæ•°æ®é›† `Dataset` çš„åˆ›å»ºã€‚

3. æ¥ç€åˆå§‹åŒ–æ¨¡å‹ `MyModel`ï¼Œæ¨¡å‹æ–‡ä»¶å­˜æ”¾åœ¨ `./src/model.py` å†…ã€‚
4. æœ€ååˆ›å»º `Trainer` ç±»å¯¹è±¡ï¼Œé‡Œé¢åŒ…å«äº†è®­ç»ƒçš„å…·ä½“å¾ªç¯ã€‚

åŸºæœ¬ä¸Šå†™å®Œä¹‹åå°±ä¸éœ€è¦å…³å¿ƒ main å‡½æ•°äº†ã€‚

## src.args

å‚æ•°é…ç½®è¿™é‡Œå…¶å®éƒ½æ˜¯ä¸€æ ·çš„ï¼Œæˆ‘åªæ˜¯æŠŠå®ƒå•ç‹¬æ‹¿å‡ºæ¥è€Œå·²ã€‚

å½“ç„¶ï¼Œæœ‰äº›äººä¹Ÿå¯èƒ½ä¹ æƒ¯ç”¨ `yaml` æ¥é…ç½®å‚æ•°ï¼Œé“ç†æ˜¯ä¸€æ ·çš„ã€‚

```py
import argparse


def get_args():
    parser = argparse.ArgumentParser(description="Training arguments.")
    # commom
    parser.add_argument('--gpu', default=-1, type=int, help='the gpu to use.')
    parser.add_argument('--mode', default='train', type=str, help='train/test')
    ...

    parser.add_argument('--dataset',
                        default='dataset-1',
                        type=str,
                        help='Choose your dataset')

    args = parser.parse_args()

    # Adjustment according to the dataset
    if args.dataset == 'dataset-1':
        args.alpha = 0.5
        args.beta = 0.5
    elif args.dataset == 'dataset-2':
        args.alpha = 0.3
        args.beta = 0.3

    return args
```

## src.dataloader

`dataloader.py` æ–‡ä»¶é‡Œé¢åªå®šä¹‰äº† `Mydataloader` ç±»ï¼Œç”¨äºæ§åˆ¶æ‰€æœ‰å’Œæ•°æ®ç›¸å…³çš„ä¸œè¥¿ã€‚

```py
class Mydataloader():
    def __init__(self, config): ...
    def load(self): ...                 # è¯»å–æ•°æ®
    def create_dataset(self): ...       # åˆ›å»ºæ•°æ®é›†ï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„ Dataset è¿›è¡ŒåŒ…è£…
    def train_dataloader(self): ...     # è®­ç»ƒé›† DataLoader
    def valid_dataloader(self): ...     # éªŒè¯é›† DataLoader
    def test_dataloader(self): ...      # æµ‹è¯•é›† DataLoader
    def split_dataset(self, data, rate): ...    # è‡ªå®šä¹‰æ•°æ®é›†åˆ†å‰²å‡½æ•°
    @staticmethod
    def read_dataset_1():...            # é’ˆå¯¹æ•°æ®é›† 1 çš„è‡ªå®šä¹‰æ•°æ®å¤„ç†å‡½æ•°
    @staticmethod
    def read_dataset_2():...            # é’ˆå¯¹æ•°æ®é›† 2 çš„è‡ªå®šä¹‰æ•°æ®å¤„ç†å‡½æ•°
```

### load() & create_dataset()

`load()` å‡½æ•°å¹¶æ²¡æœ‰å®ç°ä»€ä¹ˆå…·ä½“çš„åŠŸèƒ½ï¼Œåªæ˜¯ä¸€ä¸ªåŠ è½½æ•°æ®çš„å…¥å£ã€‚

`create_dataset()` å‡½æ•°çš„ä¸»è¦ä½œç”¨å°±æ˜¯å°†è¯»åˆ°çš„æ•°æ®åŒ…è£…æˆè‡ªå®šä¹‰çš„`Dataset`ï¼Œå¯¹æ•°æ®é›†è¿›è¡Œåˆ’åˆ†ï¼ŒåŒæ—¶å¯¹æ•°æ®è¿›è¡Œä¿å­˜ã€‚

```py
class Mydataloader():
    '''load data
    '''
    def __init__(self, config):
        self.config = config
        self.dataset_dir = Path(config.dataset_dir)
        if not self.dataset_dir.exists():
            self.dataset_dir.mkdir()

    def load(self):
        '''load dataset according to config.dataset
        '''
        if self.config.dataset.lower() == 'dataset-1':
            # do custom dataset-1 process
            self.data = Mydataloader.read_dataset_1()
            # or
            # self.data_train = Mydataloader.read_dataset_1('train')
            # self.data_valid = Mydataloader.read_dataset_1('valid')
            # self.data_test = Mydataloader.read_dataset_1('test')

        elif self.config.dataset.lower() == 'dataset-2':
            # do custom dataset-2 process
            self.data = Mydataloader.read_dataset_2()

    def create_dataset(self):
        '''create dataset according to customize dataset
        '''
        dataset_train_path = self.dataset_dir / f'{self.config.dataset}_train.pkl'
        dataset_valid_path = self.dataset_dir / f'{self.config.dataset}_valid.pkl'
        dataset_test_path = self.dataset_dir / f'{self.config.dataset}_test.pkl'
        if dataset_test_path.exists():
            if self.config.mode == 'train':
                self.dataset_train = torch.load(dataset_train_path)
                self.dataset_valid = torch.load(dataset_valid_path)
            elif self.config.mode == 'test':
                self.dataset_test = torch.load(dataset_test_path)
            return

        # create dataset
        dataset = MyDataset(self.config, self.data)

        # split dataset
        self.dataset_train, dataset_test = self.split_dataset(dataset, rate=0.8)
        self.dataset_valid, self.dataset_test = self.split_dataset(dataset_test, rate=0.5)
        torch.save(self.dataset_train, dataset_train_path)
        torch.save(self.dataset_valid, dataset_valid_path)
        torch.save(self.dataset_test, dataset_test_path)
```

### dataloader

```py
    def train_dataloader(self):
        return DataLoader(
            dataset=self.dataset_train,
            batch_size=self.config.batch_size,
            pin_memory=True,
            shuffle=True,
        )

    def valid_dataloader(self):
        return DataLoader(
            dataset=self.dataset_valid,
            batch_size=self.config.batch_size,
            shuffle=False,
        )

    def test_dataloader(self):
        return DataLoader(
            dataset=self.dataset_test,
            batch_size=self.config.batch_size,
            shuffle=False,
        )
```

ä¸ºäº†åé¢æ–¹ä¾¿ï¼Œç›´æ¥æŠŠä¸‰ä¸ª `DataLoader` å†™æˆå‡½æ•°ï¼Œè¿™æ ·åœ¨åé¢è°ƒç”¨çš„æ—¶å€™åªéœ€è¦è¿™æ ·å†™å°±è¡Œäº†ï¼š

```py
train_dl = dataloader.train_dataloader()
valid_dl = dataloader.valid_dataloader()
test_dl = dataloader.test_dataloader()
```

### custom

ä¸‹é¢è¿™ä¸‰ä¸ªå‡½æ•°æˆ‘è¿™é‡Œéƒ½æ˜¯ä¸¾äº†ä¸ªä¾‹å­ï¼ŒåŸºæœ¬éƒ½æ˜¯é’ˆå¯¹æ•°æ®é›†çš„åŠ è½½å…·ä½“å¤„ç†æ–¹æ³•ï¼š

```py
    def split_dataset(self, data, rate):
        '''custom split function
        '''
        train_size = int(rate * len(data))
        test_size = len(data) - train_size
        data_train, data_test = random_split(data, [train_size, test_size])
        return data_train, data_test

    @staticmethod
    def read_dataset_1():
        '''load the dataset here or process the dataset
        '''
        data = datasets.load_iris()
        return data

    @staticmethod
    def read_dataset_2():
        '''load the dataset here or process the dataset
        '''
        pass
```

## dataset

`dataset.py` æ–‡ä»¶é‡Œé¢å­˜æ”¾äº†è‡ªå®šä¹‰çš„æ•°æ®ç»“æ„ï¼Œè¿™ä¸ªæ–‡ä»¶è¿˜æ˜¯è¦ä¿®æ”¹ä¸ºä½ è‡ªå·±çš„ï¼Œä¹Ÿæ²¡ä»€ä¹ˆå¥½è¯´çš„ã€‚

```py
class MyDataset(Dataset):
    def __init__(self, config, data):
        super().__init__()
        self.config = config
        self.data = data

    def __len__(self):
        return len(self.data.data)

    def __getitem__(self, idx):
        feature = torch.Tensor(self.data.data)[idx]
        label = torch.LongTensor(self.data.target)[idx]
        return feature, label
```

## model

`model.py` æ–‡ä»¶åŒç†ï¼Œå­˜æ”¾æ¨¡å‹æ–‡ä»¶ï¼Œè¿™ä¸ªä¹Ÿåº”è¯¥éƒ½æ˜¯è¿™ä¹ˆåšçš„ã€‚ç®€å•è´´ä¸ªä»£ç ï¼š

```py
class MyModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.layer1 = nn.Linear(4, 50)
        self.layer2 = nn.Linear(50, 50)
        self.layer3 = nn.Linear(50, 3)

    def forward(self, x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        x = F.softmax(self.layer3(x), dim=1)
        return x
```

## trainer

`trainer.py` æ–‡ä»¶é‡Œå®šä¹‰äº† `Trainer` ç±»ï¼ŒåŒ…å«ä¸‹é¢å‡ ä¸ªå‡½æ•°ï¼š

```py
class Trainer:
    def __init__(self, config, logger=None, gpu=-1): ...
    def train(self, model, dataloader): ...
    def train_epoch(self, epoch, model, train_dl, optimizer, scheduler, criterion): ...
    @torch.no_grad()
    def valid_epoch(self, epoch, model, valid_dl, criterion): ...
    @torch.no_grad()
    def test(self, model, dataloader, model_path): ...
```

### train

åœ¨ `main` å‡½æ•°é‡Œè°ƒç”¨äº† `train()` åï¼Œå°±ä¼šè¿›è¡Œä¸‹é¢çš„æ­¥éª¤ï¼š

1. å®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
2. åˆ›å»º DataLoader
3. å‡†å¤‡æ¨¡å‹
4. è¿›å…¥å¾ªç¯
   1. `train_epoch()`
   2. `valid_epoch()`
   3. ä¿å­˜æ¨¡å‹

```py
def train(self, model, dataloader):
    # prepare optimizer and criterion
    optimizer = torch.optim.Adam(model.parameters(), lr=self.config.learning_rate)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)
    criterion = torch.nn.CrossEntropyLoss()

    # prepare dataloader
    train_dl = dataloader.train_dataloader()
    valid_dl = dataloader.valid_dataloader()

    # prepare model
    model = model.to(self.device)

    self.logger.info('start training...')
    self.logger.info(f'device: {self.device}')
    for epoch in range(self.config.epochs):
        # train epoch
        self.train_epoch(epoch, model, train_dl, optimizer, scheduler, criterion)

        # valid epoch
        self.valid_epoch(epoch, model, valid_dl, criterion)

        # save model
        torch.save(model.state_dict(), self.model_dir / f"model_{epoch+1}.pkl")
    self.logger.info('training done!')
```

### train_epoch

è¿™é‡Œä»¥ `train_epoch` ä¸ºä¾‹ï¼Œç†Ÿæ‚‰è®­ç»ƒè¿‡ç¨‹çš„è¯ä½ å°±ä¼šå‘ç°è¿™é‡Œå…¶å®å°±æ˜¯æ­£å¸¸çš„è®­ç»ƒæµç¨‹ï¼ŒåŸºæœ¬ä»€ä¹ˆä¹Ÿæ²¡å˜ã€‚

```py
def train_epoch(self, epoch, model, train_dl, optimizer, scheduler, criterion):
    model.train()
    train_loss = []
    tbar = tqdm(train_dl, total=len(train_dl), desc='Training')
    for idx, dl in enumerate(tbar):
        feature, label = dl
        feature = feature.to(self.device)
        label = label.to(self.device)

        optimizer.zero_grad()
        pred = model(feature)
        loss = criterion(pred, label)
        loss.backward()
        optimizer.step()

        train_loss.append(loss.item())
        # update bar
        tbar.set_description(f'Epoch [{epoch + 1}/{self.config.epochs}]')
        tbar.set_postfix(loss=f'{np.mean(train_loss):.4f}',
                            lr=scheduler.get_last_lr()[0])
    scheduler.step()
```

## utils

å·¥å…·ç±»å°±ä¸è¯´äº†ï¼Œä¸çŸ¥é“å¾€å“ªæ”¾å°±æ”¾è¿™é‡Œã€‚

## å…¶ä»–

å¦å¤–ï¼Œä¸ªäººä¸€èˆ¬è¿˜ä¼šæœ‰ä¸€ä¸ª `preprocess.py` æ–‡ä»¶ç”¨æ¥åšæ•°æ®å¤„ç†ï¼Œå› ä¸ºè¿™ä¹Ÿæ˜¯å¾ˆå¤§çš„ä¸€éƒ¨åˆ†ï¼Œè¿™æ ·åœ¨ `dataloader` é‡Œé¢åŠ è½½æ•°æ®çš„æ—¶å€™å°±ä¼šå¹²å‡€å¾ˆå¤šã€‚

## æ€»ç»“

çœ‹å®Œä¹‹åä½ ä¼šå‘ç°ï¼Œå…¶å®å°±æ˜¯åŸç”Ÿçš„ PyTorchï¼Œä¹Ÿå®Œå…¨æ²¡æœ‰å˜åŒ–ï¼Œæˆ‘åªæ˜¯æŒ‰æˆ‘è‡ªå·±æ¯”è¾ƒå–œæ¬¢ã€ä¹ æƒ¯çš„æ–¹å¼å¯¹åŠŸèƒ½è¿›è¡Œåˆ†é—¨åˆ«ç±»ï¼Œè®©æ•´ç†çš„æµç¨‹å˜å¾—æ›´åŠ ç®€å•å’Œæ¸…æ™°ã€‚è¿™æ ·æˆ‘ä¸‹æ¬¡å†™æ¨¡å‹çš„æ—¶å€™åŸºæœ¬åªéœ€è¦å…³æ³¨æ•°æ®éƒ¨åˆ†ä»¥åŠæ¨¡å‹éƒ¨åˆ†ï¼Œé’ˆå¯¹æ¨¡å‹è®­ç»ƒæµç¨‹ä¸Šçš„ä¸œè¥¿å¤§éƒ¨åˆ†åªéœ€è¦å¤ç”¨è¿™ä¸ªæ¨¡å¼å°±å¯ä»¥äº†ã€‚
